{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "data = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "data['PRICE'] = housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('PRICE', axis=1)\n",
    "y = data['PRICE']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Traditional Machine Learning Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Comp2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "lr_mae = mean_absolute_error(y_test, lr_pred)\n",
    "lr_rmse = mean_squared_error(y_test, lr_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Comp2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "rf_rmse = mean_squared_error(y_test, rf_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Comp2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 768us/step - loss: 1.5339 - val_loss: 0.4616\n",
      "Epoch 2/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.4079 - val_loss: 0.4260\n",
      "Epoch 3/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.3746 - val_loss: 0.4037\n",
      "Epoch 4/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 0.3523 - val_loss: 0.3932\n",
      "Epoch 5/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.4152 - val_loss: 0.3655\n",
      "Epoch 6/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.3370 - val_loss: 0.3590\n",
      "Epoch 7/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.3395 - val_loss: 0.3512\n",
      "Epoch 8/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.3087 - val_loss: 0.3461\n",
      "Epoch 9/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.3159 - val_loss: 0.3470\n",
      "Epoch 10/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - loss: 0.2983 - val_loss: 0.3445\n",
      "Epoch 11/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.2902 - val_loss: 0.3806\n",
      "Epoch 12/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - loss: 0.3804 - val_loss: 0.3272\n",
      "Epoch 13/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 0.2992 - val_loss: 0.3363\n",
      "Epoch 14/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.3086 - val_loss: 0.3271\n",
      "Epoch 15/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.2909 - val_loss: 0.3322\n",
      "Epoch 16/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.2968 - val_loss: 0.3260\n",
      "Epoch 17/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.2943 - val_loss: 0.3432\n",
      "Epoch 18/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.2744 - val_loss: 0.3278\n",
      "Epoch 19/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.2844 - val_loss: 0.3201\n",
      "Epoch 20/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.2838 - val_loss: 0.3161\n",
      "Epoch 21/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step - loss: 0.2906 - val_loss: 0.3231\n",
      "Epoch 22/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492us/step - loss: 0.2851 - val_loss: 0.3166\n",
      "Epoch 23/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 0.2673 - val_loss: 0.3143\n",
      "Epoch 24/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - loss: 0.2858 - val_loss: 0.3098\n",
      "Epoch 25/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - loss: 0.2681 - val_loss: 0.3126\n",
      "Epoch 26/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.2746 - val_loss: 0.3118\n",
      "Epoch 27/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.2742 - val_loss: 0.3068\n",
      "Epoch 28/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.2702 - val_loss: 0.3061\n",
      "Epoch 29/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.2619 - val_loss: 0.3204\n",
      "Epoch 30/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.2659 - val_loss: 0.3142\n",
      "Epoch 31/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.2973 - val_loss: 0.3014\n",
      "Epoch 32/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.2637 - val_loss: 0.3099\n",
      "Epoch 33/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.2597 - val_loss: 0.3042\n",
      "Epoch 34/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.2575 - val_loss: 0.3113\n",
      "Epoch 35/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 0.2572 - val_loss: 0.2958\n",
      "Epoch 36/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.2539 - val_loss: 0.2951\n",
      "Epoch 37/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.2487 - val_loss: 0.3017\n",
      "Epoch 38/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - loss: 0.2488 - val_loss: 0.3056\n",
      "Epoch 39/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.2797 - val_loss: 0.3305\n",
      "Epoch 40/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - loss: 0.2553 - val_loss: 0.2960\n",
      "Epoch 41/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 0.2630 - val_loss: 0.3183\n",
      "Epoch 42/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - loss: 0.2570 - val_loss: 0.2945\n",
      "Epoch 43/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 0.2532 - val_loss: 0.2914\n",
      "Epoch 44/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - loss: 0.2535 - val_loss: 0.3046\n",
      "Epoch 45/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.2524 - val_loss: 0.2935\n",
      "Epoch 46/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - loss: 0.2531 - val_loss: 0.2907\n",
      "Epoch 47/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - loss: 0.2457 - val_loss: 0.2895\n",
      "Epoch 48/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.2421 - val_loss: 0.2859\n",
      "Epoch 49/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 0.2473 - val_loss: 0.2952\n",
      "Epoch 50/50\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 0.2582 - val_loss: 0.3014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x261fbe9b740>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train_scaled, y_train, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('california_housing_model.h5')\n",
    "model.save('california_housing_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Comp2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nn_pred = model.predict(X_test_scaled)\n",
    "nn_mae = mean_absolute_error(y_test, nn_pred)\n",
    "nn_rmse = mean_squared_error(y_test, nn_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MAE: 0.533200130495656, RMSE: 0.7455813830127761\n",
      "Random Forest MAE: 0.32750093968023264, RMSE: 0.5027936350215934\n",
      "Neural Network MAE: 0.3782377092429002, RMSE: 0.5392801737672307\n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear Regression MAE: {lr_mae}, RMSE: {lr_rmse}\")\n",
    "print(f\"Random Forest MAE: {rf_mae}, RMSE: {rf_rmse}\")\n",
    "print(f\"Neural Network MAE: {nn_mae}, RMSE: {nn_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Metrics and results:\n",
    "MAE (Mean Absolute Error) (smaller value = better result): This metric measures the average absolute difference between the predicted values and the actual values. It gives us an idea of how much, on average, our predictions are off from the actual values.\n",
    "\n",
    "RMSE (Root Mean Squared Error (smaller value = better result)): This metric measures the square root of the average of the squared differences between the predicted values and the actual values. It gives us an idea of the magnitude of error in our predictions, with more emphasis on larger errors due to the squaring process.\n",
    "\n",
    "Results Interpretation:\n",
    "\n",
    "- Linear Regression:\n",
    "MAE: 0.5332\n",
    "RMSE: 0.7456\n",
    "Linear Regression is a simple model that tries to fit a straight line through the data. In this case, it has an MAE of about 0.5332, meaning on average, the predictions are off by about 0.5332 units from the actual values. The RMSE is 0.7456, which indicates the spread of the errors and suggests that there are some larger errors in the predictions.\n",
    "\n",
    "- Random Forest:\n",
    "MAE: 0.3275\n",
    "RMSE: 0.5028\n",
    "Random Forest is an ensemble method that builds multiple decision trees and combines their results. It performs better than Linear Regression here, with a lower MAE of 0.3275. This means the average prediction error is smaller. The RMSE is also lower at 0.5028, indicating fewer and smaller large errors compared to Linear Regression.\n",
    "\n",
    "- Neural Network:\n",
    "MAE: 0.3782\n",
    "RMSE: 0.5393\n",
    "The Neural Network is a more complex model that can capture non-linear relationships in the data. It has an MAE of 0.3782, which is higher than Random Forest but lower than Linear Regression. The RMSE of 0.5393 suggests that while it performs better than Linear Regression, it doesn't perform as well as Random Forest in terms of minimizing large errors.\n",
    "\n",
    "## Summary:\n",
    "\n",
    "Random Forest performed the best overall, with the lowest MAE and RMSE, indicating it made the most accurate predictions with the smallest errors.\n",
    "Neural Network performed better than Linear Regression but not as well as Random Forest.\n",
    "Linear Regression had the highest errors, indicating it was the least accurate model for this dataset.\n",
    "In practical terms, this means that for the California Housing Dataset, Random Forest was able to capture the relationships in the data most effectively, followed by the Neural Network, and then Linear Regression. This comparison shows the importance of trying different models and comparing their performance to select the best one for your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When would neural networks be better than traditional machine learning methods?\n",
    "\n",
    "As we can see, in some cases traditional machine learning approaches like random forest or linear regression can provide better results than neural networks...however:\n",
    "\n",
    "## Neural networks often provide better results compared to traditional machine learning models like linear regression or random forests under certain conditions. Here are some factors and scenarios where neural networks might outperform other models:\n",
    "\n",
    "1. Complex and Non-Linear Relationships:\n",
    "When the data has complex, non-linear relationships: Neural networks are highly capable of capturing complex patterns and interactions between features. If your data has such characteristics, a neural network can potentially model these relationships better than linear regression or even random forests.\n",
    "2. Large Amount of Data:\n",
    "When there is a large amount of data: Neural networks, particularly deep learning models, perform better with large datasets. They require a significant amount of data to learn effectively and generalize well.\n",
    "3. Feature Engineering:\n",
    "When feature engineering is challenging: Neural networks can automatically learn representations of the data through their multiple layers, reducing the need for manual feature engineering.\n",
    "4. High-Dimensional Data:\n",
    "When dealing with high-dimensional data: Neural networks can handle large numbers of features and can be particularly effective in domains like image and text data where traditional models might struggle.\n",
    "5. Model Flexibility:\n",
    "When flexibility is needed: Neural networks can be customized with different architectures (e.g., CNNs for images, RNNs for sequences) to fit the specific nature of the problem.\n",
    "Examples and Improvements for Neural Networks:\n",
    "To leverage the strengths of neural networks and potentially achieve better results, consider the following:\n",
    "\n",
    "1. Hyperparameter Tuning:\n",
    "Experiment with different architectures: Try different numbers of layers and neurons.\n",
    "Adjust learning rates and batch sizes: Fine-tune the optimizer settings.\n",
    "2. Regularization Techniques:\n",
    "Dropout: Helps prevent overfitting by randomly setting a fraction of input units to 0 at each update during training.\n",
    "L2 Regularization: Adds a penalty to the loss function to reduce overfitting.\n",
    "3. Advanced Architectures:\n",
    "Convolutional Neural Networks (CNNs): For image data.\n",
    "Recurrent Neural Networks (RNNs) or Long Short-Term Memory networks (LSTMs): For sequential data.\n",
    "4. More Data:\n",
    "Collect more training data: If possible, gather more data to train the neural network, which often helps in improving performance.\n",
    "5. Data Augmentation:\n",
    "Data augmentation: Techniques to artificially increase the size of the training set by creating modified versions of existing data (common in image and text data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
